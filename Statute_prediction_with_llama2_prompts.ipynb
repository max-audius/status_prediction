{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzJlMEbeo5R2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "The LLama 2 project is a collection of pretrained and fine-tuned generative text models designed specifically for dialogue use cases. Ranging from 7 billion to 70 billion parameters, these models outperform open-source chat models on various benchmarks and demonstrate comparable performance to popular closed-source models in terms of helpfulness and safety.\n",
        "\n",
        "## **LLama 2 13B-chat Model**\n",
        "The llama.cpp file within this repository serves the objective of running the LLaMA model with 4-bit integer quantization on MacBook. It is a C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Initially developed as a web chat example, it now acts as a development playground for ggml library features.\n",
        "\n",
        "## **GGML Library**\n",
        "The GGML (Generative Generative Models Library) is a C library for machine learning. It facilitates the distribution of large language models (LLMs) and uses quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version numbers, hyperparameters, vocabulary, and weights. The vocabulary includes tokens for language generation, and the weights determine the LLM's size. Quantization reduces precision to optimize resource usage.\n",
        "\n",
        "## **Quantized Models from Hugging Face Community**\n",
        "The Hugging Face community provides quantized models that efficiently utilize the model on the T4 GPU. Ensure to consult reliable sources before using any model. Among the available variations, we are interested in those based on the GGLM library.\n",
        "\n",
        "The Llama-2-13B-GGML model, and its variations, can be found [here](https://huggingface.co/models?search=llama%202%20ggml).\n"
      ],
      "metadata": {
        "id": "ncRKj_Wgo7wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ydG22_jrp7Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation**"
      ],
      "metadata": {
        "id": "OekLAT2rp_dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required **Packages**"
      ],
      "metadata": {
        "id": "5ofNp-3XqO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4\n"
      ],
      "metadata": {
        "id": "QF7WAT35qT7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Required **Libraries**"
      ],
      "metadata": {
        "id": "-_D4DlA2q5Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n"
      ],
      "metadata": {
        "id": "lwDhNjxzq7El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Download the **Model**"
      ],
      "metadata": {
        "id": "KcTm9pwCrJgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n"
      ],
      "metadata": {
        "id": "9BkE9oKvrLN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load the **Model**"
      ],
      "metadata": {
        "id": "WLMMeTI-rPWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # CPU cores\n",
        "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32  # Change this value based on your model and your GPU VRAM pool.\n",
        ")"
      ],
      "metadata": {
        "id": "GdtkM6JWrTUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Load Test **Dataset**"
      ],
      "metadata": {
        "id": "ITvwjwOqrdN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to load the test dataset from Google Drive\n",
        "def load_test_dataset(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        dataset = [json.loads(line) for line in file]\n",
        "    return dataset\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset_path = \"/content/drive/MyDrive/test.jsonl\"  # Update with your actual path\n",
        "test_data = load_test_dataset(test_dataset_path)\n"
      ],
      "metadata": {
        "id": "NBZ7iuMKrisd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Generate **Predictions**"
      ],
      "metadata": {
        "id": "irt9-GigsEC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(prompt):\n",
        "    response = lcpp_llm(prompt=prompt, max_tokens=256, temperature=0.5, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n",
        "    return response\n",
        "\n",
        "# Generate predictions for each text in the test dataset\n",
        "predicted_labels = []\n",
        "for example in test_data[:300]:\n",
        "    text = ' '.join(example['text'])[:512]\n",
        "    prompt = f\"predict the relevant statutes {text}\"\n",
        "    response = generate_predictions(prompt)\n",
        "    response_text = response['choices'][0]['text']\n",
        "    predicted_label = response_text.splitlines()[0]\n",
        "    predicted_labels.append(predicted_label)"
      ],
      "metadata": {
        "id": "FTbpZRzbsJlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Evaluate **Performance**"
      ],
      "metadata": {
        "id": "-gUc7zwksNLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(predictions, true_labels):\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    true_labels_binary = mlb.fit_transform(true_labels)\n",
        "    predictions_binary = mlb.transform(predictions)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_binary, predictions_binary)\n",
        "    precision = precision_score(true_labels_binary, predictions_binary, average=\"macro\")\n",
        "    recall = recall_score(true_labels_binary, predictions_binary, average=\"macro\")\n",
        "    f1 = f1_score(true_labels_binary, predictions_binary, average=\"macro\")\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Extract true labels from the test dataset\n",
        "true_labels = [example['labels'][0] for example in test_data[:300]]\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, precision, recall, f1 = evaluate_model(predicted_labels, true_labels)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Macro Precision: {precision}\")\n",
        "print(f\"Macro Recall: {recall}\")\n",
        "print(f\"Macro F1: {f1}\")\n"
      ],
      "metadata": {
        "id": "cO4Ez4pEsRje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}